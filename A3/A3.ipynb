{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85e442cc",
   "metadata": {},
   "source": [
    "# Assignment 3: Text processing with LSTM in PyTorch\n",
    "\n",
    "*Author:* Thomas Adler\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors.\n",
    "\n",
    "In this assignment you will a train an LSTM to generate text. To be able to feed text into (recurrent) neural networks we first have to choose a good representation. There are several options to do so ranging from simple character embeddings to more sophisticated approaches like [word embeddings](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa) or [token embeddings](https://medium.com/@_init_/why-bert-has-3-embedding-layers-and-their-implementation-details-9c261108e28a). We will use a character embedding in this assignment. \n",
    "\n",
    "Character embeddings work as follows. First we define an alphabet, a set of characters that we want to be able to represent. To feed a character into our network we use a one-hot vector. The dimension of this vector is equal to the size of our alphabet and the \"hot\" position indicates the character we want to represent. While this is logically a decent representation (all characters have the same norm, are orthogonal to one another, etc.) it is inefficient in terms of memory because we have to store a lot of zeros. In the first layer of our network we will multiply our one-hot vector with a weight matrix, i.e. we compute the preactivation by a matrix-vector product of the form $We_i$, where $e_i$ is the $i$-th canonical basis vector. This operation corresponds to selecting the $i$-th column of $W$. So an efficient implementation is to perform a simple lookup operation in $W$. This is how embedding layers work also for word or token embeddings. They are learnable lookup tables. \n",
    "\n",
    "## Exercise 1: Encoding characters\n",
    "\n",
    "Write a class `Encoder` that implements the methods `__init__` and `__call__`. The method `__init__` takes a string as argument that serves as alphabet. The method `__call__` takes one argument. If it is a string then it should return a sequence of integers as `torch.Tensor` of shape  representing the input string. Each integer should represent a character of the alphabet. The alphabet consists of the characters matched by the regex `[a-z0-9 .!?]`. If the input text contains characters that are not in the alphabet, then `__call__` should either remove them or map them to a corresponding character that belongs to the alphabet. If the argument is a `torch.Tensor`, then the method should return a string representation of the input, i.e. it should function as decoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17f16ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([32, 33, 31, 22, 27, 20, 33, 18, 27, 32, 28, 31], dtype=torch.int32)\n",
      "stringtensor\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "class Encoder:\n",
    "    \n",
    "    def __init__(self,alphabet_string:str):\n",
    "        self.alphabet_list=list(set(alphabet_string.lower()))\n",
    "        self.alphabet_list.sort()\n",
    "        pattern = re.compile(fr\"[a-z0-9 .!?]\")\n",
    "        \n",
    "        self.invalid_chars = [_ for _ in self.alphabet_list if not pattern.match(_)] #set of invalid characters\n",
    "        self.alphabet_list = [_ for _ in self.alphabet_list if pattern.match(_)] #set of valid \n",
    "        \n",
    "        self.alphabet_size = len(self.alphabet_list)\n",
    "     \n",
    "        self.alphabet = {char:char_idx for char_idx,char in enumerate(self.alphabet_list)}\n",
    "    \n",
    "    def alphabet_size(self):\n",
    "        return len([_ for _ in self.alphabet_list if pattern.match(_)])\n",
    "    \n",
    "    def __call__(self,arg):\n",
    "        if type(arg)==torch.Tensor:\n",
    "            return ''.join([self.alphabet_list[int(_.item())] for _ in torch.squeeze(arg)])\n",
    "        elif type(arg)==str:\n",
    "            return torch.Tensor([self.alphabet[_] for _ in arg if _ not in self.invalid_chars ]).int()\n",
    "\n",
    "        \n",
    "encoder = Encoder(\"abcdefghijklmnopqrstuvwxyz0123456789.!?'$% \")\n",
    "tensor_ = encoder(\"stringt'en'sor\")\n",
    "string_tensor = encoder(encoder(\"stringt'en'sor\"))\n",
    "\n",
    "print(tensor_)\n",
    "print(string_tensor)\n",
    "print(encoder.alphabet_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a819873",
   "metadata": {},
   "source": [
    "## Exercise 2: Pytorch Dataset\n",
    "\n",
    "Write a class `TextDataset` that derives from `torch.utlis.data.Dataset`. It should wrap a text file and utilize it for training with pytorch. Implement the methods `__init__`, `__len__`, `__getitem__`. The method `__init__` should take a path to a text file as string and an integer `l` specifying the length of one sample sequence. The method `__len__` takes no arguments and should return the size of the dataset, i.e. the number of sample sequences in the dataset. The method `__getitem__` should take an integer indexing a sample sequence and should return that sequence as a `torch.Tensor`. The input file can be viewed as one long sequence. The first sample sequence consists of the characters at positions `0..l-1` in the input file. The second sequence consists of the characters at positions `l..2*l-1` and so on. That is, the samples of our dataset are non-overlapping sequences. The last incomplete sequence may be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9df917ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "3319\n",
      "things and the husbands one of the great abusers of the world? give me a break. give me a break.       give me a break. so the last person she wants to run against is me.   now heres the story. look heres the story. its very simple. so i announced  and i w\n",
      "torch.Size([19, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self,file_path:str,l:int):\n",
    "        \n",
    "        self.l = l\n",
    "        \n",
    "        with open(file_path, encoding=\"utf8\") as text_file:#get file from filepath\n",
    "            self.raw_data = text_file.read().lower()\n",
    "        #read file\n",
    "        \n",
    "        self.raw_data = self.raw_data.replace(\"\\n\",\" \").replace(\"\\r\",\" \")\n",
    "        #linebreaks create whitespaces\n",
    "        \n",
    "        self.encoder = Encoder(self.raw_data)\n",
    "        self.encoded_data = self.encoder(self.raw_data)\n",
    "        #create and apply encoder\n",
    "        \n",
    "        self.encoded_data = torch.narrow(self.encoded_data,0,0,self.encoded_data.shape[0]-(self.encoded_data.shape[0]%l))\n",
    "        #last incomplete sequence is dropped\n",
    "        \n",
    "        self.sequences = torch.reshape(self.encoded_data,(np.floor(self.encoded_data.shape[0]/self.l).astype(int),l))\n",
    "        #break data into sequences\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences) #or something\n",
    "    \n",
    "    def __getitem__(self,sequence_index):\n",
    "        \"\"\"\n",
    "        call __getitem__ like:\n",
    "        TextDataset[get_argument]\n",
    "        \"\"\"\n",
    "        return self.sequences[sequence_index]\n",
    "\n",
    "trump_dataset = TextDataset(r\"trump\\trump_train.txt\",l=256)\n",
    "print(trump_dataset.encoder.alphabet_list)\n",
    "print(len(trump_dataset)) #__len__ method\n",
    "print(trump_dataset.encoder(trump_dataset[668]))\n",
    "\n",
    "print(trump_dataset[123:142].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e5724f",
   "metadata": {},
   "source": [
    "## Exercise 3: The Model\n",
    "\n",
    "Write a class `NextCharLSTM` that derives from `torch.nn.Module` and takes `alphabet_size`, the `embedding_dim`, and the `hidden_dim` as arguments. It should consist of a `torch.nn.Embedding` layer that maps the alphabet to embeddings, a `torch.nn.LSTM` that takes the embeddings as inputs and maps them to hidden states, and a `torch.nn.Linear` output layer that maps the hidden states of the LSTM back to the alphabet. Implement the methods `__init__` that sets up the module and `forward` that takes an input sequence and returns the logits (i.e. no activation function on the output layer) of the model prediction at every time step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "459fe907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256])\n",
      "torch.Size([14, 256, 40])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "class NextCharLSTM(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,alphabet_size:int,embedding_dim:int,hidden_dim:int):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=alphabet_size,embedding_dim=embedding_dim)\n",
    "        \"\"\"\n",
    "        from the docs:\n",
    "        num_embeddings (int) – size of the dictionary of embeddings\n",
    "        embedding_dim (int) – the size of each embedding vector\n",
    "        --> embedding dim is given as input\n",
    "        \"\"\"\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=hidden_dim,num_layers=1,batch_first=True) #just one hidden layer is default\n",
    "        \n",
    "        self.linear = nn.Linear(in_features=hidden_dim,out_features=alphabet_size)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self,input_sequence):\n",
    "        embedded_sequence=self.embedding(input_sequence)\n",
    "\n",
    "        lstm_out,hn= self.lstm(embedded_sequence)\n",
    "        #<-- activation function here?\n",
    "        logits = self.linear(lstm_out)\n",
    "                \n",
    "        return logits\n",
    "        \n",
    "\n",
    "model = NextCharLSTM(alphabet_size = trump_dataset.encoder.alphabet_size,\n",
    "                            embedding_dim = trump_dataset.encoder.alphabet_size,\n",
    "                            hidden_dim = 25 #arbitrary\n",
    "                           )\n",
    "print(trump_dataset[786].shape)\n",
    "out = model.forward(trump_dataset[786:800])\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7cb10d",
   "metadata": {},
   "source": [
    "## Exercise 4: Training/Validation Epoch\n",
    "\n",
    "Write a function `epoch` that takes a `torch.utils.data.DataLoader`, a `NextCharLSTM`, and a `torch.optim.Optimizer` as arguments, where the last one might be `None`. If the optimizer is `None`, then the function should validate the model. Otherwise it should train the model for next-character prediction in the many-to-many setting. That is, given a sequence `x` of length `l`, the input sequence is `x[:l-1]` and the corresponding target sequence is `x[1:]`. The function should perform one epoch of training/validation and return the loss values of each mini batch as a numpy array. Use the cross-entropy loss function for both training and validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13f33250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "def get_input_and_label(batch):\n",
    "    x = batch[:,:-1]\n",
    "    y = batch[:,1:]\n",
    "    return x,y\n",
    "\n",
    "def epoch(dataloader:DataLoader,model_:NextCharLSTM,optimizer:torch.optim.Optimizer=None):\n",
    "    \n",
    "    loss_seq = []\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if optimizer is None:\n",
    "        for batch_idx,batch in enumerate(dataloader):\n",
    "        \n",
    "            x_batch,y_batch =get_input_and_label(batch)\n",
    "\n",
    "            pred = model_.forward(x_batch)        \n",
    "            loss = loss_fn(pred.swapaxes(2,1),y_batch.long())\n",
    "\n",
    "            loss_seq.append(loss.item())\n",
    "\n",
    "            return np.average(loss_seq)\n",
    "    \n",
    "    for batch_idx,batch in enumerate(dataloader):\n",
    "        progress_percent = np.floor((batch_idx/len(dataloader))*100).astype(int)\n",
    "        if progress_percent%10==0:\n",
    "            print(f\"\\tProgress: {progress_percent}%\")\n",
    "        \n",
    "        x_batch,y_batch =get_input_and_label(batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model_.forward(x_batch)\n",
    "        \n",
    "        loss = loss_fn(pred.swapaxes(2,1),y_batch.long())\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_seq.append(loss.item())\n",
    "        \n",
    "        \n",
    "    return np.array(loss_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cb856c",
   "metadata": {},
   "source": [
    "## Exercise 5: Model Selection\n",
    "\n",
    "Usually, we would now train and validate our model with different hyperparameters to see which setting performs best. However, this pretty expensive in terms of compute so we will provide you with a setting that should work quite well. Train your model for 30 epochs using `torch.optim.Adam`. Validate your model after every epoch and persist the model that performs best on the validation set using `torch.save`. Visualize and discuss the training and validation progress. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8987ae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sequence_length = 100\n",
    "batch_size = 256\n",
    "embedding_dim = 8\n",
    "hidden_dim = 512\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 5\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "\n",
    "train_data = TextDataset(r\"trump\\trump_train.txt\",l=100)\n",
    "valid_data = TextDataset(r\"trump\\trump_val.txt\",l=100)\n",
    "\n",
    "\n",
    "model = NextCharLSTM(alphabet_size = train_data.encoder.alphabet_size,\n",
    "                            embedding_dim = embedding_dim,\n",
    "                            hidden_dim = hidden_dim\n",
    "                           )\n",
    "\n",
    "\n",
    "dataloader = DataLoader(train_data,batch_size=256,shuffle=True)\n",
    "val_dataloader = DataLoader(valid_data,batch_size=256)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "loss_array,val_array = np.array([]),np.array([])\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "    best_model_performance = np.average(epoch(val_dataloader,model)) #initial validation\n",
    "except Exception as ex:\n",
    "    best_model_performance = 10 #a large loss\n",
    "    print(\"Loading Model failed: \",str(ex))\n",
    "\n",
    "if False:\n",
    "    for _ in range(num_epochs):\n",
    "        print(f\"Epoch number {_+1} of {num_epochs}\")\n",
    "        loss_array = np.append(loss_array,np.average(epoch(dataloader,model,optimizer)))\n",
    "        val_array = np.append(val_array,np.average(epoch(val_dataloader,model)))\n",
    "        print(f\"Validation Accuracy: {np.average(epoch(val_dataloader,model))}\")\n",
    "        if np.average(epoch(val_dataloader,model)) < best_model_performance:\n",
    "            print(\"New best model\")\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        else:\n",
    "            print(\"Model performs worse than best model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6978b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_array.flatten(),label=\"Training Data\")\n",
    "plt.plot(val_array.flatten(),label=\"Validation Data\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c68d0c",
   "metadata": {},
   "source": [
    "## Exercise 6: Top-$k$ Accuracy\n",
    "\n",
    "Write a function `topk_accuracy` that takes a list of integers $k$, a model, and a data loader and returns the top-$k$ accuracy of the model on the given data set for each $k$. A sample is considered to be classified correctly if the true label appears in the top-$k$ classes predicted by the model. Then load the best model from the previous exercise using `torch.load` and plot its top-$k$ accuracy as a function of $k$ for all possible values of $k$. Discuss the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe1f70cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "def topk_accuracy(k_list:list,model:NextCharLSTM,dataloader:DataLoader):\n",
    "    \n",
    "    topk_accuracy_dict = {k:[] for k in k_list}\n",
    "    for batch_idx,batch in enumerate(dataloader):\n",
    "\n",
    "        x_batch,y_batch =get_input_and_label(batch)\n",
    "\n",
    "        pred = model.forward(x_batch)        \n",
    "        \n",
    "        for k,k_accuracies in topk_accuracy_dict.items():\n",
    "            pred_topk = torch.topk(pred,k,dim=2,sorted=True)\n",
    "            topk_acc = torch.any(pred_topk.indices == y_batch.unsqueeze(2),dim=2)\n",
    "            topk_acc_sequence_mean = torch.mean(topk_acc.type(torch.float),dim=1) #mean accuracies of the individual sequences\n",
    "            topk_acc_batch_mean = torch.mean(topk_acc_sequence_mean) #mean over the whole batch\n",
    "            \n",
    "            k_accuracies.append(topk_acc_batch_mean)\n",
    "        \n",
    "    for k,k_accuracies in topk_accuracy_dict.items():\n",
    "        topk_accuracy_dict[k]=np.mean(topk_accuracy_dict[k]) #mean over all batches\n",
    "        \n",
    "    return topk_accuracy_dict\n",
    "\n",
    "loaded_model = NextCharLSTM(alphabet_size = train_data.encoder.alphabet_size,\n",
    "                            embedding_dim = embedding_dim,\n",
    "                            hidden_dim = hidden_dim\n",
    "                           )\n",
    "loaded_model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "loaded_model.eval()\n",
    "\n",
    "topk_out = topk_accuracy(list(range(1,41)),loaded_model,val_dataloader)\n",
    "\n",
    "plt.plot(topk_out.values())\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Top-k Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1efe72",
   "metadata": {},
   "source": [
    "The Top-k Accuracy of the best model increases with k. This makes intuitive sense. If k is equal to one, the Top-k accuracy is the percentage of correct first guesses. If k is equal to the size of the alphabet, every guess is right because the correct character is always in the top-k guesses.\n",
    "\n",
    "The graph also shows that the correct letter is likely within the top-10 letters predicted by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fc1f27",
   "metadata": {},
   "source": [
    "## Exercise 7: Deterministic Text Generation\n",
    "\n",
    "In this exercise we utilize the trained network to generate novel text. To do this, take some seed text, which can be chosen by the user, and feed it to the network. Subsequently, extrapolate new text by always appending the top-1 character according to the model prediction to the input sequence. Discuss the quality of your model as a text generator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ce3ca64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last year they want the want the want the want the want the want the want the want the want the want the want the want the want the want the want the want the want the want the want the want the want the want the wa\n"
     ]
    }
   ],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "def determ_pred(self,input_str:str,pred_len:int):\n",
    "    prediction = encoder(input_str)[None,:]\n",
    "    for _ in range(pred_len):\n",
    "        iter_prediction = self.forward(prediction)\n",
    "        iter_prediction_idx = torch.max(iter_prediction,dim=2).indices\n",
    "        \n",
    "        prediction = torch.cat((prediction,iter_prediction_idx[:,-1].unsqueeze(0)),dim=1)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "NextCharLSTM.determ_pred = determ_pred\n",
    "\n",
    "output = model.determ_pred(\"last year they \",200)\n",
    "print(encoder(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfca9e2",
   "metadata": {},
   "source": [
    "## Exercise 8: Probabilistic Text Generation\n",
    "\n",
    "Utilize your trained model as text generator as in the previous exercise but with one difference. Instead of always choosing the top-1 character make a probabilistic choice. The network prediction constitutes a probability distribution over the alphabet. Choose the next character by sampling from this distribution. Compare the results to those of the previous exercise and discuss the observed differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac5d1a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "america is inn ictoofd b t saut weris. in le. i waue the relaldy. wapericg jf the kmow a linist of whar what w0\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "\n",
    "def prob_pred(self,input_str:str,pred_len:int):\n",
    "    prediction = encoder(input_str)[None,:]\n",
    "    for _ in range(pred_len):\n",
    "        iter_prediction = self.forward(prediction)\n",
    "        prob_distribution = Categorical(logits=iter_prediction[:,-1].unsqueeze(0))\n",
    "        sample = prob_distribution.sample()[0][:1]\n",
    "                \n",
    "        prediction = torch.cat((prediction,sample.unsqueeze(0)),dim=1)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "NextCharLSTM.prob_pred=prob_pred\n",
    "\n",
    "output = model.prob_pred(\"america is \",100)\n",
    "print(encoder(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0166f488",
   "metadata": {},
   "source": [
    "## Exercise 9: Visualize Neurons\n",
    "\n",
    "Visualize the value of the 512 neurons while the trained model processes some user-defined text. Take a look at the last figure of [this blog](https://openai.com/blog/unsupervised-sentiment-neuron/) (which is also a good read) to get an idea of how to do the visualization. You can install and use the package `colorama` for that. Can you figure out certain repsonsibilities of certain neurons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a979acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13])\n",
      "torch.Size([1, 13, 512])\n",
      "yellow light\n",
      "red light\n",
      "red light\n",
      "red light\n",
      "red light\n",
      "red light\n",
      "red light\n",
      "red light\n",
      "red light\n",
      "red light\n",
      "red light\n",
      "red light\n",
      "red light\n",
      "out_string:  test sequence\n",
      "test string\n",
      "red string\n"
     ]
    }
   ],
   "source": [
    "# provides readable names for ANSI escape sequences\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "def forward_hidden(self,input_sequence):\n",
    "    embedded_sequence=self.embedding(input_sequence)\n",
    "\n",
    "    lstm_out,hn= self.lstm(embedded_sequence)\n",
    "    #<-- activation function here?\n",
    "    logits = self.linear(lstm_out)\n",
    "\n",
    "    return logits, hn[0]\n",
    "\n",
    "def get_all_hidden(self,input_str):\n",
    "    input_seq = encoder(input_str)[None,:]\n",
    "    print(input_seq.shape)\n",
    "    prediction,hidden = self.forward_hidden(input_seq[None,:,0])\n",
    "    \n",
    "    for _ in range(1,input_seq.shape[-1]):\n",
    "        \n",
    "        iter_prediction,iter_hidden = self.forward_hidden(input_seq[:_])\n",
    "        hidden = torch.cat((hidden,iter_hidden),dim=1)\n",
    "                    \n",
    "    return hidden\n",
    "\n",
    "\n",
    "NextCharLSTM.forward_hidden = forward_hidden\n",
    "NextCharLSTM.get_all_hidden = get_all_hidden\n",
    "\n",
    "input_string = 'test sequence'\n",
    "neuron_idx = 256\n",
    "\n",
    "all_hidden = model.get_all_hidden(input_string)\n",
    "print(all_hidden.shape)\n",
    "\n",
    "out_string = \"\"\n",
    "for char_idx,char in enumerate(input_string):\n",
    "    if all_hidden[0,char_idx,neuron_idx] > .33:\n",
    "        print(\"green light\")\n",
    "        out_string += (Back.GREEN +char)\n",
    "    elif all_hidden[0,char_idx,neuron_idx] < -.33:\n",
    "        print(\"red light\")\n",
    "        out_string += (Back.YELLOW +char)\n",
    "    else:\n",
    "        print(\"yellow light\")\n",
    "        out_string += (Back.RED +char)\n",
    "        \n",
    "        \n",
    "#what the fuck is wrong?\n",
    "\n",
    "print(\"out_string: \",out_string)\n",
    "\n",
    "print(Back.RED + \"test string\")\n",
    "print(Fore.RED + \"red string\") #!??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab14725",
   "metadata": {},
   "source": [
    "## Bonus Exercise (3 Points):\n",
    "\n",
    "Adapt your code from the previous exercises such that the model runs in the many-to-one setting, i.e., it should read `l-1` characters of a sample sequence and predict the `l`-th character. Train/validate the model in the many-to-one setting and compare it to the many-to-many setting in terms of top-$k$ accuracy on the validation set and probabilistic text generation. Visualize your results. What are the pros and cons? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78195916",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3b7b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Playing around with it ###\n",
    "\n",
    "train_data = TextDataset(r\"dontpanic\\dontpanic_train.txt\",l=100)\n",
    "valid_data = TextDataset(r\"dontpanic\\dontpanic_validate.txt\",l=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b0c7dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model at 1.7259238958358765 loss\n"
     ]
    }
   ],
   "source": [
    "dontpanic_num_epochs = 100\n",
    "\n",
    "dontpanic_model = NextCharLSTM(alphabet_size = train_data.encoder.alphabet_size,\n",
    "                            embedding_dim = embedding_dim,\n",
    "                            hidden_dim = hidden_dim\n",
    "                           )\n",
    "\n",
    "dontpanic_dataloader = DataLoader(train_data,batch_size=256,shuffle=True)\n",
    "dontpanic_val_dataloader = DataLoader(valid_data,batch_size=256)\n",
    "dontpanic_optimizer = torch.optim.Adam(dontpanic_model.parameters(),lr=learning_rate)\n",
    "\n",
    "\n",
    "try:\n",
    "    dontpanic_model.load_state_dict(torch.load(\"dontpanic_best_model.pt\"))\n",
    "    dontpanic_best_model_performance = np.average(epoch(dontpanic_val_dataloader,dontpanic_model)) #initial validation\n",
    "    print(f\"Loaded model at {dontpanic_best_model_performance} loss\")\n",
    "except Exception as ex:\n",
    "    dontpanic_best_model_performance = 10 #a large loss \n",
    "    print(\"Loading Model failed: \",str(ex))\n",
    "\n",
    "dontpanic_loss_array,dontpanic_val_array = np.array([]),np.array([])\n",
    "\n",
    "if False:\n",
    "    for _ in range(dontpanic_num_epochs):\n",
    "        print(f\"Epoch number {_+1} of {dontpanic_num_epochs}\")\n",
    "        dontpanic_loss_array = np.append(dontpanic_loss_array,np.average(epoch(dontpanic_dataloader,dontpanic_model,dontpanic_optimizer)))\n",
    "        dontpanic_val_array = np.append(dontpanic_val_array,np.average(epoch(dontpanic_val_dataloader,dontpanic_model)))\n",
    "        print(f\"Validation Accuracy: {np.average(epoch(dontpanic_val_dataloader,dontpanic_model))}\")\n",
    "        print(f\"Training Accuracy: {dontpanic_loss_array[-1]}\")\n",
    "        if np.average(epoch(dontpanic_val_dataloader,dontpanic_model)) < dontpanic_best_model_performance:\n",
    "            print(\"New best model\")\n",
    "            torch.save(dontpanic_model.state_dict(), \"dontpanic_best_model.pt\")\n",
    "        else:\n",
    "            print(\"Model performs worse than best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5872aaf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(dontpanic_loss_array.flatten(),label=\"Training Data\")\n",
    "plt.plot(dontpanic_val_array.flatten(),label=\"Validation Data\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15093f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arthur was a see should said ford and the stare of the stare of the stare of the stare of the stare of the stare of the stare of the stare of the stare of the stare of the stare of the stare of the stare of the \n",
      "arthur was a solvids all s i  propled by this pelyor  bilaching hes think is ip roke.  they apored to jatre this his than but you yturs hhat.er a bzurk into  manictly and then what sack outs.  thert?  ay dong to\n"
     ]
    }
   ],
   "source": [
    "dontpanic_model.load_state_dict(torch.load(\"dontpanic_best_model.pt\"))\n",
    "\n",
    "dontpanic_output = dontpanic_model.determ_pred(\"arthur was \",200)\n",
    "print(encoder(dontpanic_output))\n",
    "\n",
    "dontpanic_output = dontpanic_model.prob_pred(\"arthur was \",200)\n",
    "print(encoder(dontpanic_output))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
