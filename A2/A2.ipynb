{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Long Short-Term Memory\n",
    "\n",
    "*Author:* Thomas Adler\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Analysis of the Jordan network\n",
    "\n",
    "In the previous assignment we found that the Jordan network is incapable of memorizing information over longer time spans. Now we want to delve deeper into why this is case and how to solve this problem. To learn long-term dependencies, it is necessary that the gradient carries the error signal backwards in time. For instance, if the output at time $t$ depends on the input at time $s$ with $s < t$, then the Jacobian \n",
    "$$\n",
    "J(t, s) = \\frac{\\partial a(t)}{\\partial a(s)}\n",
    "$$\n",
    "is responsible for carrying the error signals from time $t$ backwards to time $s$. Calculate this Jacobian for the Jordan network and elaborate on the numerical stability of $J(t, s)$ for long time spans, i.e., when $t-s$ becomes large. Why is the Jordan network incapable of learning long-term dependencies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "$$\n",
    "J(t,s) = \\frac{\\partial a(t)}{\\partial a(s)}, s<t\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a(t)}{\\partial a(s)} = \\frac{\\partial a(t)}{\\partial s(t)}\\frac{\\partial s(t)}{\\partial \\hat y(t-1)}\\frac{\\partial \\hat{y}(t-1)}{\\partial z(t-1)}\\frac{\\partial z(t-1)}{\\partial a(t-1)}\\frac{\\partial a(t-1)}{\\partial a(s)}\n",
    "$$\n",
    "applying the above extension recursively:\n",
    "$$\n",
    "\\frac{\\partial a(t)}{\\partial a(s)} = \\prod_{i=1}^{t-s} \\frac{\\partial a((t+1)-i)}{\\partial s((t+1)-i)}\\frac{\\partial s((t+1)-i)}{\\partial \\hat y(t-i)}\\frac{\\partial \\hat{y}(t-i)}{\\partial z(t-i)}\\frac{\\partial z(t-i)}{\\partial a(t-i)}\n",
    "$$\n",
    "the Jacobian Matrix $J(t,s)$ is then:\n",
    "$$\n",
    "\\frac{\\partial a(t)}{\\partial a(s)} = \\prod_{i=1}^{t-s} (1-tanh^2(s(t))) \\, R \\, \\sigma '(z) \\, V\n",
    "$$\n",
    "\n",
    "Which scales exponentially with $t-s$. This that the Jacobian is numerically unstable  when $t-s$ becomes large. The Jordan network is incabable of learning long-term dependencies because the error either explodes or vanishes as dependencies grow further apart in time.\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure depicts the LSTM architecture (without forget gate, which was introduced later). \n",
    "\n",
    "<img src=\"lstm_noFG.png\" alt=\"LSTM\" width=\"600\"/>\n",
    "\n",
    "We have\n",
    "$$\n",
    "a(t) = \\varphi(W_a x(t) + R_a h(t-1) + b_a),\n",
    "$$\n",
    "where $a \\in \\{z, i, f, o\\}$ and $\\varphi$ is either sigmoid (for $i, f, o$) or tanh (for $z$). We alter the notation of the figure in that we write $h(t)$ instead of $y(t)$, which lets us use the latter for the output variable as we are used to. The LSTM forward rule is \n",
    "$$\n",
    "c(t) = c(t-1) + z(t) \\odot i(t) \\\\\n",
    "h(t) = \\tanh(c(t)) \\odot o(t).\n",
    "$$\n",
    "To obtain predictions $\\hat y(t)$ we facilitate an output layer\n",
    "$$\n",
    "\\hat y(t) = \\sigma(W_y h(t) + b_y).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Forward pass of the gates\n",
    "\n",
    "Consider the layer \n",
    "$$\n",
    "a = \\varphi(W x + R h + b).\n",
    "$$\n",
    "The modules $a \\in \\{z, i, o\\}$ are called cell input, input gate, and output gate, respectively. The cell input uses $\\varphi = \\tanh$ whereas input gate and output gate use $\\varphi = \\sigma$. Implement the forward pass of the class `Gate` by implementing the methods `__init__` and `forward`. The method `__init__` should initialize the parameters uniformly in $[-0.01, 0.01]$. The method `forward` should implement the forward logic of the module. The activation function $\\varphi$ should be exchangeable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "def tanh_der(x):\n",
    "    return (1-(tanh(x)**2))\n",
    "\n",
    "def sigm_der(x):\n",
    "    return (sigmiod(x)*(1-sigmoid(x)))\n",
    "\n",
    "class Gate(object):\n",
    "\n",
    "    def __init__(self,I,D,T,af,af_der):\n",
    "        \n",
    "                   \n",
    "        self.a=np.zeros((T+1,I)) #bc there is timestep -1\n",
    "        \n",
    "        self.W = np.random.uniform(-.01,.01,size=(D,I))\n",
    "        self.R = np.random.uniform(-.01,.01,size=(I,I))\n",
    "        self.b = np.random.uniform(-.01,.01,size=(I))\n",
    "        \n",
    "        self.af = af\n",
    "        self.af_der = af_der\n",
    "\n",
    "    \n",
    "    def forward(self, x, h, t): #af-->activation function\n",
    "        self.a[t] = self.af(self.W.T@x+self.R@h+self.b) #h[t] because h starts at t=-1\n",
    "\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Gradients for the gates\n",
    "\n",
    "To train a gate module, we need the gradients of the loss with respect to the parameters $W, R, b$. Further, if there are layers below that need training, then we also need the gradients with respect to $x$ and $h$. Given the gradient $\\nabla_a L$, derive expressions for the gradients w.r.t $W, R, b, x, h$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "From the Forum: \"in this exercise we consider the gate as an isolated module without any time indices\"\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\n",
    "\\nabla_{a}L=\\frac{\\partial L}{\\partial a}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{R}L=\\frac{\\partial L}{\\partial a}\\frac{\\partial a}{\\partial R}= \\frac{\\partial L}{\\partial a}\\frac{\\partial a}{\\partial \\varphi} h\n",
    "$$\n",
    " \n",
    "\n",
    "$$\n",
    "\\nabla_{W}L=\\frac{\\partial L}{\\partial a}\\frac{\\partial a}{\\partial \\varphi}\\frac{\\partial \\varphi}{ \\partial W} = \\frac{\\partial L}{\\partial a}\\frac{\\partial a}{\\partial \\varphi} x\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{b}L=\\frac{\\partial L}{\\partial a}\\frac{\\partial a}{\\partial \\varphi}\\frac{\\partial \\varphi}{ \\partial b} = \\frac{\\partial L}{\\partial a}\\frac{\\partial a}{\\partial \\varphi}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{x}L=\\frac{\\partial L}{\\partial a}\\frac{\\partial a}{\\partial \\varphi}\\frac{\\partial \\varphi}{ \\partial x} = \\frac{\\partial L}{\\partial a}\\frac{\\partial a}{\\partial \\varphi} W\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{h}L=\\frac{\\partial L}{\\partial a}\\frac{\\partial a}{\\partial \\varphi}\\frac{\\partial \\varphi}{ \\partial R} = \\frac{\\partial L}{\\partial a}\\frac{\\partial a}{\\partial \\varphi} R\n",
    "$$\n",
    "\n",
    "With $\\frac{\\partial a}{\\partial \\varphi}$ being either $1-tanh(a)^2$ or $\\sigma(a) (1-\\sigma(a))$ depending on what $\\varphi$ is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Backward pass of the gates\n",
    "\n",
    "Implement the backward pass of the class `Gate` by implementing the methods `zero_grad`, `backward`, and `update`. The method `zero_grad` should initialize/overwrite the gradients w.r.t. the parameters to zero. The method `backward` should take $\\nabla_a L$ as argument and return $\\nabla_x L$ and $\\nabla_h L$. Moreover, it should add $\\nabla_W L, \\nabla_R L, \\nabla_b L$ to the gradient buffers. The method update should perform a gradient-descent step to update the parameters using a learning rate $\\eta$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "def zero_grad(self):\n",
    "    self.nabla_W_L=0\n",
    "    self.nabla_R_L=0\n",
    "    self.nabla_b_L=0\n",
    "    self.nabla_h_L=0\n",
    "    self.nabla_x_L=0\n",
    "\n",
    "def backward(self,nabla_a_L, t):\n",
    "    self.nabla_W_L+=nabla_a_L@(self.af_der(self.a[t])@h)\n",
    "    self.nabla_R_L+=nabla_a_L@(self.af_der(self.a[t])@x)\n",
    "    self.nabla_b_L+=nabla_a_L@(self.af_der(self.a[t]))\n",
    "    \n",
    "    nabla_L_x=nabla_a_L@np.diag(self.af_der(self.a[t]))@self.W\n",
    "    nabla_L_h=nabla_a_L@np.diag(self.af_der(self.a[t]))@self.R\n",
    "\n",
    "    return nabla_L_x,nabla_L_h\n",
    "\n",
    "def update(self,eta):\n",
    "    self.W+=eta*self.nabla_W_L\n",
    "    self.R+=eta*self.nabla_R_L\n",
    "    self.b+=eta*self.nabla_b_L\n",
    "\n",
    "Gate.zero_grad=zero_grad\n",
    "Gate.backward=backward\n",
    "Gate.update=update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Forward pass of LSTM\n",
    "\n",
    "Implement the forward pass of the class `LSTM` using the `Gate` module. Again, it should have the methods `__init__` and `forward` using the same initialization scheme as before. The method `forward` should evaluate and return $L(\\hat y(T), y(T))$ where $L$ is the binary cross-entropy loss function and $T$ is the index of the last sequence element. Make sure to store all activations which are needed for the backward pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(object):\n",
    "    \n",
    "    def __init__(self,D,I,K,T):\n",
    "        \n",
    "        self.T=T\n",
    "        \n",
    "        self.z = Gate(I,D,T,af=np.tanh,af_der=tanh_der)\n",
    "        self.i = Gate(I,D,T,af=sigmoid,af_der=sigm_der)\n",
    "        self.o = Gate(I,D,T,af=sigmoid,af_der=sigm_der)\n",
    "        self.gates=[self.i,self.o,self.z]\n",
    "        \n",
    "        self.h = np.zeros((T+1,I))\n",
    "        self.c=np.zeros((T+1,I))\n",
    "        \n",
    "        self.W=np.zeros((K,I))\n",
    "        self.R=np.zeros((K,I))\n",
    "        self.b=np.zeros(K)\n",
    "    \n",
    "    def loss(self,y,y_hat):\n",
    "        return -(y*np.log(y_hat))-(1-y)*(np.log(1-y_hat))\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = np.zeros((T,K))\n",
    "        self.c = np.zeros((T+1,I))\n",
    "        self.h = np.zeros((T+1,I))\n",
    "        \n",
    "        self.y_hat = np.zeros((T,K))\n",
    "        \n",
    "        for t in range(self.T):\n",
    "            self.i.forward(x[t],self.h[t],t)\n",
    "            self.o.forward(x[t],self.h[t],t)\n",
    "            self.z.forward(x[t],self.h[t],t)\n",
    "            \n",
    "            self.c[t+1] = self.c[t]+self.z.a[t]@self.i.a[t] #ð‘(ð‘¡âˆ’1)[scalar]+ð‘§(ð‘¡)[vector]âŠ™ð‘–(ð‘¡)[vector] HADAMARD!!\n",
    "            self.h[t+1] = np.tanh(self.c[t+1])@self.o.a[t] #â„Ž(ð‘¡)[vector]=tanh(ð‘(ð‘¡))[scalar]âŠ™ð‘œ(ð‘¡)[vector] HADAMARD!!\n",
    "\n",
    "            self.y_hat[t] = sigmoid(self.W@self.h[t]+self.b) #ð‘¦Ì‚(ð‘¡)[vector]=ðœŽ(ð‘Šð‘¦â„Ž(ð‘¡)[vector]+ð‘ð‘¦[vector])\n",
    "        \n",
    "        return self.loss(y,self.y_hat[-1])\n",
    "\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "lstm=LSTM(1,2,3,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Backward pass of LSTM with BPTT\n",
    "\n",
    "Realize the backward pass of LSTM by implementing the methods `zero_grad`, `backward`, and `update`. Equations for the gradients can be found in the lecture script section 3.1 \"Backpropagation for LSTM\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "def zero_grad(self):\n",
    "    self.grad_L_h = np.zeros(T,I) #in the script this is y\n",
    "    self.grad_L_o = np.zeros(T,I)\n",
    "    self.grad_L_i = np.zeros(T,I)\n",
    "    self.grad_L_z = np.zeros(T,I)\n",
    "    self.grad_L_c = np.zeros(T+1,I)\n",
    "    \n",
    "    self.delta_i = np.zeros(T,I)\n",
    "    self.delta_o = np.zeros(T,I)\n",
    "    self.delta_z = np.zeros(T,I)\n",
    "    \n",
    "    self.grad_W = np.zeros_like(self.W)\n",
    "    self.grad_b = np.zeros_like(self.b)\n",
    "    \n",
    "    self.grad_L_h_direct = np.zeros(T,I)\n",
    "    \n",
    "\n",
    "def backward(self,I,T):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    grad y_hat is necessary because the derivatives in the script start from h \n",
    "    (which in the script is denoted by y)\n",
    "    --> der L(t)/h(t) = L(t)/yhat(t) yhat(t)/h(t) = L(t)/sigmoid(out(t)) sigmoid(out(t))/out(t) out(t)/h(t)\n",
    "    \n",
    "    L/h is zero for all t<T bc many-to-one\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #grad_L_h_direct --> sigmoidal function is already incorporated in the gradient of the loss\n",
    "    self.grad_L_h_direct[T] = (self.y - self.y_hat[T])@self.W # D - I x D @ I --> vector of length D; I x D @ D --> vector of length I\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    grad_i_h,grad_o_h,grad_z_h = np.zeros(I,I),np.zeros(I,I),np.zeros(I,I)\n",
    "    grad_L_c = np.zeros(I)\n",
    "    \n",
    "    self.grad_L_h[T]=self.grad_L_h_direct[T] # because loss is only at time T and not after T\n",
    "    \n",
    "    for t_shift in range(T):\n",
    "        t=T-t_shift\n",
    "\n",
    "        self.grad_L_o[t] = self.grad_L_h[t]@np.diag(np.tanh(self.c[t])) # I x I @ I --> I\n",
    "        \n",
    "        grad_L_c = self.grad_L_h[t]@np.diag(self.o.a[t]*tanh_der(c[t]))+grad_L_c\n",
    "        \n",
    "        self.grad_L_i[t] = grad_L_c@np.diag(self.z.a[t])\n",
    "        self.grad_L_z[t] = grad_L_c@np.diag(self.i.a[t])\n",
    "                              \n",
    "        # h has a range of T+1 because of h at time t = -1\n",
    "        # x has range of T \n",
    "        grad_i_h = self.i.backward(self.grad_L_i[t], t)\n",
    "        grad_o_h = self.o.backward(self.grad_L_o[t], t)\n",
    "        grad_z_h = self.z.backward(self.grad_L_z[t], t)\n",
    "        \n",
    "        \n",
    "        self.grad_L_h[t-1] = (self.grad_L_h_direct[t] \n",
    "                                + self.grad_L_i[t]@grad_i_h\n",
    "                                + self.grad_L_o[t]@grad_o_h\n",
    "                                + self.grad_L_z[t]@grad_z_h)\n",
    "\n",
    "        self.delta_i[t] = self.grad_L_i[t]@np.diag(self.i.af_der(self.i.W@self.x[t]+self.i.R@self.h[t-1]))\n",
    "        self.delta_o[t] = self.grad_L_o[t]@np.diag(self.o.af_der(self.o.W@self.x[t]+self.o.R@self.h[t-1]))\n",
    "        self.delta_z[t] = self.grad_L_z[t]@np.diag(self.z.af_der(self.z.W@self.x[t]+self.z.R@self.h[t-1]))\n",
    "\n",
    "        self.grad_L_Ri += self.delta_i[t]@h[t-1].T\n",
    "        self.grad_L_Ro += self.delta_o[t]@h[t-1].T\n",
    "        self.grad_L_Rz += self.delta_z[t]@h[t-1].T\n",
    "        \n",
    "        self.grad_L_Wi += self.delta_i[t]@x[t].T\n",
    "        self.grad_L_Wo += self.delta_o[t]@x[t].T\n",
    "        self.grad_L_Wz += self.delta_z[t]@x[t].T\n",
    "        \n",
    "        self.grad_L_bi += self.delta_i\n",
    "        self.grad_L_bo += self.delta_o\n",
    "        self.grad_L_bz += self.delta_z\n",
    "    \n",
    "    #finally the gradients with respect to the output layer can be calculated by benefit of joining Cross-Entropy-Loss function and the sigmoid\n",
    "    #same way as in Assignment 1\n",
    "    self.grad_W += (self.y - self.y_hat[T])@h[T]\n",
    "    self.grad_b += (self.y - self.y_hat[T])\n",
    "    \n",
    "def update(self,eta):\n",
    "    self.i.R += eta*self.grad_L_Ri\n",
    "    self.o.R += eta*self.grad_L_Ro\n",
    "    self.z.R += eta*self.grad_L_Rz\n",
    "\n",
    "    self.i.W += eta*self.grad_L_Wi\n",
    "    self.o.W += eta*self.grad_L_Wo\n",
    "    self.z.W += eta*self.grad_L_Wz\n",
    "\n",
    "    self.i.b += eta*self.grad_L_bi\n",
    "    self.o.b += eta*self.grad_L_bo\n",
    "    self.z.b += eta*self.grad_L_bz\n",
    "\n",
    "    self.W += eta*self.grad_W\n",
    "    self.b += eta*self.grad_b\n",
    "\n",
    "LSTM.zero_grad = zero_grad\n",
    "LSTM.backward = backward\n",
    "LSTM.update = update\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: LSTM training\n",
    "\n",
    "Train an LSTM with 32 hidden units on the task from assignment 1 with a sequence length of 100. Tune the number of update steps and the learning rate. After training, evaluate the model on 1000 sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69314718])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "D = 1\n",
    "I = 32\n",
    "K = 1\n",
    "T = 100\n",
    "\n",
    "np.random.seed(0xDEADBEEF)#is it supposed to spell dead beef? moooh!\n",
    "\n",
    "def generate_data(T):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "    x_prime=np.random.choice([1,-1],1,p=[.5,.5])\n",
    "    gaussian=np.random.normal(0,.2,size=(1,(T-1)))\n",
    "    x = np.insert(gaussian,0,x_prime.T,axis=1)\n",
    "    y = ((x_prime+1)/2).astype(int)#hacky\n",
    "    yield [x.T,y]\n",
    "\n",
    "lastima = LSTM(D,I,K,T)\n",
    "x,y = generate_data(T).__next__()\n",
    "lastima.forward(x,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8: The LSTM learning method and RTRL\n",
    "\n",
    "Analyzing full BPTT for LSTM we find that the recurrent connections through the gates causes most of the intricacy of the backward logic, while the simple inner recurrent connections are responsible for carrying the error signals over long time spans. The *LSTM learning method* truncates the gradient of these outer recurrent connections. In other words, the gates treat $h(t)$ as if they were external inputs and disregard their dependence on the model. This simplifies the gradients and makes RTRL feasible. \n",
    "\n",
    "Since $h(t)$ are treated as external inputs, the key to RTRL is the recursion\n",
    "$$\n",
    "\\frac{\\partial c(t)}{\\partial \\theta} = \\sum_{s=1}^t \\frac{\\partial c(t)}{\\partial \\theta(s)} = \\frac{\\partial c(t)}{\\partial \\theta(t)} + \\frac{\\partial c(t)}{\\partial c(t-1)} \\frac{\\partial c(t-1)}{\\partial \\theta},\n",
    "$$\n",
    "where $\\theta$ is the parameter vector that contains all the model parameters in one large vector. The parameters are shared in time and $\\theta(t)$ denotes their usage at time $t$. Above recursion lets us collect the part of the gradient that depends on the past during forward pass. Due to the recurrent weights the size of $\\theta$ is $O(I^2)$ and therefore $\\frac{\\partial c(t)}{\\partial \\theta}$ is $O(I^3)$. The matrix product on the right-hand side raises the computational complexity of RTRL to $O(I^4)$, which is the reason why RTRL is infeasible for most recurrent architectures. \n",
    "\n",
    "Let, e.g., $w_{jk}^i$ denote the element in the $j$-th row and $k$-th column of the matrix $W$ belonging to the input gate $i$. Show that $\\frac{\\partial c(t)}{\\partial \\theta}$ for the LSTM learning method has the form \n",
    "$$\n",
    "\\frac{\\partial c_n(t)}{\\partial w_{jk}^i} = z_n(t) i_n(t)(1-i_n(t)) x_k(t) [n=j] \\qquad\n",
    "\\frac{\\partial c_n(t)}{\\partial r_{jk}^i} = z_n(t) i_n(t)(1-i_n(t)) h_k(t-1) [n=j] \\qquad\n",
    "\\frac{\\partial c_n(t)}{\\partial b_j^i} = z_n(t) i_n(t)(1-i_n(t)) [n=j] \\\\\n",
    "\\frac{\\partial c_n(t)}{\\partial w_{jk}^z} = i_n(t) (1-z_n(t)^2) x_k(t) [n=j] \\qquad\n",
    "\\frac{\\partial c_n(t)}{\\partial r_{jk}^z} = i_n(t) (1-z_n(t)^2) h_k(t-1) [n=j] \\qquad\n",
    "\\frac{\\partial c_n(t)}{\\partial b_j^z} = i_n(t) (1-z_n(t)^2) [n=j],\\\\\n",
    "$$\n",
    "where $[n=j]$ is the Iverson bracket that evaluates to 1 if the expression inside is true and to 0 otherwise. What is the complexity of RTRL for the LSTM learning method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9: Prepare LSTM for RTRL\n",
    "\n",
    "Write a class `LSTM_RTRL` and implement the methods `__init__`, `zero_grad`, `update`. Do not use the `Gate` class this time but implement the gates directly so they can be trained via RTRL. Make sure to initialize all weights and gradient buffers accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10: Implement RTRL for the LSTM learning method\n",
    "\n",
    "Add a method `forward` to the class `LSTM_RTRL` that processes one time step of an input sequence and updates the RTRL gradient buffers using the LSTM learning method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 11: LSTM training with RTRL\n",
    "\n",
    "Again, train the LSTM on the task from assignment 1 using real-time recurrent learning in combination with the LSTM learning method. Start with a sequence length of 1. What is the maximum sequence length for which the LSTM can learn the task (at reasonable computational cost)? Compare the training behavior to that with BPTT. Explain possible differences.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
