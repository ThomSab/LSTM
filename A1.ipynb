{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: The Jordan Network\n",
    "\n",
    "*Author:* Thomas Adler\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors.\n",
    "\n",
    "## Exercise 1: Initializing the network\n",
    "Consider the Jordan network (with $f(x) = \\tanh(x) = (e^x - e^{-x})(e^x + e^{-x})^{-1}$ and $\\varphi(x) = \\sigma(x) = (1+e^{-x})^{-1}$ and transposed weight matrices compared to the lecture notes)\n",
    "$$\n",
    "s(t) = W x(t) + R \\hat y(t-1) \\\\\n",
    "a(t) = \\tanh(s(t)) \\\\\n",
    "z(t) = V a(t) \\\\\n",
    "\\hat y(t) = \\sigma(z(t))\n",
    "$$\n",
    "for $t \\in \\mathbb{N}, x(t) \\in \\mathbb{R}^{D}, s(t) \\in \\mathbb{R}^{I}, a(t) \\in \\mathbb{R}^{I}, z(t) \\in \\mathbb{R}^K, \\hat y(t) \\in \\mathbb{R}^K$ and $W, R, V$ are matrices of appropriate sizes and $\\hat y(0) = 0$. \n",
    "\n",
    "Write a function `init` that takes a `model` and integers $D, I, K$ as arguments and stores the matrices $W, R, V$ as members `model.W`, `model.R`, `model.V`, respectively. The matrices should be `numpy` arrays of appropriate sizes and filled with random values that are uniformly distributed between -0.01 and 0.01. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W:\n",
      " [[-0.00399088 -0.00216915  0.00311664  0.00128196 -0.00384173]\n",
      " [-0.00738732 -0.00549979 -0.00661492  0.00725356 -0.00164437]\n",
      " [-0.00468331  0.00220966 -0.00311014 -0.00336296 -0.00089176]]\n",
      "R:\n",
      " [-0.00999651 -0.00403475 -0.00040842  0.00590663 -0.00773346]\n",
      "V:\n",
      " [[-0.00650846]\n",
      " [ 0.00261202]\n",
      " [ 0.00654857]\n",
      " [-0.00657531]\n",
      " [-0.00049171]]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "def tanh(x):\n",
    "    return(1/((np.exp(x)-np.exp(-x))*(np.exp(x)+np.exp(-x))))\n",
    "\n",
    "def sigma(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "class Obj(object):\n",
    "    pass\n",
    "\n",
    "def init(model, D, I, K): #dunder pls\n",
    "    model.W = np.random.uniform(low=-0.01,high=0.01,size=(D,I))\n",
    "    model.R = np.random.uniform(low=-0.01,high=0.01,size=(I))\n",
    "    model.V = np.random.uniform(low=-0.01,high=0.01,size=(I,K))\n",
    "\n",
    "    t = 5 #any value of t\n",
    "    test_x = np.random.uniform(size=(D))\n",
    "    try:\n",
    "        model.W.T @ test_x\n",
    "        model.R.T*tanh(t)\n",
    "        model.V.T*tanh(t)\n",
    "    except Exception as e:\n",
    "        print(\"Model init fail\")\n",
    "        \n",
    "Obj.init=init\n",
    "\n",
    "T, D, I, K = 10, 3, 5, 1\n",
    "\n",
    "model = Obj()\n",
    "model.init(D, I, K)\n",
    "print(\"W:\\n\",model.W)\n",
    "print(\"R:\\n\",model.R)\n",
    "print(\"V:\\n\",model.V)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Numerical stability of the binary cross-entropy loss function\n",
    "\n",
    "We will use the binary cross-entropy loss function to train our RNN, which is defined as \n",
    "$$\n",
    "L(\\hat y, y) = -y \\log \\hat y - (1-y) \\log (1-\\hat y) \\quad \\text{where} \\quad \\hat y = \\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "is the sigmoid function. Its argument $z$ is called *logit*. For reasons of numerical stability it is better to let the model emit the logit and incorporate the sigmoid function into the loss function. Explain why this is the case and how we can gain numerical stability by combining the two functions into one. *Hint:* Find a numerically stable version of the function $f(z) = \\log(1+e^{z})$ and rewrite $L(z,y)$ in terms of $f(z)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANSWER\n",
    "\n",
    "If the model makes a large error st. z is far below zero, the sigmoid function might put out a float that is small enough to become a numerical zero in the execution of the code. This becomes problematic as the sigmoid output is passed to the loss function where the log of $\\hat y$ becomes $- \\infty$ and thus numerically unstable.\n",
    "We thus integrate the sigmoidal function into the loss function:\n",
    "\n",
    "$$\n",
    "L(z, y) = -y \\log ( \\frac{1}{1+e^{-z}} ) - (1-y) \\log (1-( \\frac{1}{1+e^{-z}} )) \\quad =\n",
    "$$\n",
    "$$\n",
    "= -y\\log(\\frac{1}{1 + e^{-z}}) - (1 - y)\\log(\\frac{e^{-z}}{1 + e^{-z}})\\\\\n",
    "= y  \\log(1 + e^{-z}) + (1 - y)  (-\\log(e^{-z}) + \\log(1 + e^{-z}))\\\\\n",
    "= y  \\log(1 + e^{-z}) + (1 - y)  (z + \\log(1 + e^{-z})\\\\\n",
    "= (1 - y)  z + \\log(1 + e^{-z})\\\\\n",
    "= z - z  y + \\log(1 + e^{-z})\\\\\n",
    "$$\n",
    "\n",
    "If z is negative and large the output of $e^{-z}$ can cause numerical overflow.\n",
    "In order to prevent this we can:\n",
    "\n",
    "$$\n",
    "  z - zy + \\log(1 + e^{-z})\\\\\n",
    "= \\log(e^{z}) - zy + \\log(1 + e^{-z})\\\\\n",
    "= - zy + \\log(1 + e^{z})\\\\\n",
    "$$\n",
    "\n",
    "or:\n",
    "\n",
    "$$\n",
    "= - zy + f(z)\\\\\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: The forward pass\n",
    "Write a function `forward` that takes a `model`, a sequence of input vectors $(x(t))_{t=1}^T$, and a label `y` as arguments. The inputs will be represented as a `numpy` array of shape `(T, D)`. It should execute the behavior of the Jordan network and evaluate the numerically stablilized binary cross-entropy loss at the end of the sequence and return the resulting loss value. Store the sequence of hidden activations $(a(t))_{t=1}^T$ and the sequence of logits $(z(t))_{t=1}^T$ into `model.a` and `model.z`, respectively, using the same representation scheme as for the inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.55812082966621"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_fn(y_hat,y):\n",
    "    return -(y*np.log(y_hat)-(1-y)*np.log(1-y_hat))\n",
    "\n",
    "def forward(self, x, y):\n",
    "    self.a = [np.zeros(I)]\n",
    "    self.z = []\n",
    "    self.y_hat = []\n",
    "    \n",
    "    for t,x_t in enumerate(x):#defining t to start at one but keeping a zero-th a makes this tricky\n",
    "        s = self.W.T@x[t] + self.R.T@self.a[-1]\n",
    "        self.a.append(tanh(s))\n",
    "        self.z.append(self.V.T@self.a[-1]) #a[-1] is now the new a\n",
    "    self.y_hat=list(map(sigma,self.z))\n",
    "    loss_sequence = np.array([ loss_fn(y_hat[0],y) for y_hat in self.y_hat ])\n",
    "\n",
    "    return np.average(loss_sequence)\n",
    "        \n",
    "\n",
    "Obj.forward = forward\n",
    "model.forward(np.random.uniform(-1, 1, (T, D)), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: The computational graph\n",
    "\n",
    "Visualize the functional graph of the Jordan network unfolded in time. The graph should show at least 3 consecutive time steps. Use the package `networkx` in combination with `matplotlib` to draw a directed graph with labelled nodes and edges. If you need help take a look at [this guide](https://networkx.guide/visualization/basics/). Make sure to arrange the nodes in a meaningful way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "G=nx.DiGraph()\n",
    "T = 5\n",
    "\n",
    "positions={}\n",
    "edge_labels={}\n",
    "for _ in range(1,T+1):\n",
    "    G.add_edge(f\"yhat({_-1})\",f\"a({_})\")\n",
    "    positions[f\"a({_})\"]=[_+1,0]\n",
    "    edge_labels[(f\"yhat({_-1})\",f\"a({_})\")]=\"R\"\n",
    "    G.add_edge(f\"x({_})\",f\"a({_})\")\n",
    "    positions[f\"x({_})\"]=[_+1,-1]\n",
    "    edge_labels[(f\"x({_})\",f\"a({_})\")]=\"W\"\n",
    "    G.add_edge(f\"a({_})\",f\"yhat({_})\")\n",
    "    positions[f\"yhat({_-1})\"]=[_,+1]\n",
    "    edge_labels[(f\"a({_})\",f\"yhat({_})\")]=\"V\"\n",
    "              \n",
    "positions[f\"yhat({T})\"]=[_+1,+1]    \n",
    "\n",
    "nx.draw_networkx_edge_labels(G,pos=positions,edge_labels=edge_labels,font_color='blue')\n",
    "\n",
    "nx.draw(G,with_labels=True,pos=positions,node_size=1750,node_color=\"#C1C2D4\",alpha =1,edge_color='k',linewidths=1)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Derivative of the loss\n",
    "\n",
    "Calculate the derivative of the binary cross-entropy loss function $L(z, y)$ with respect to the logit $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANSWER\n",
    "\n",
    "Binary cross-entropy loss function is\n",
    "$$\n",
    "L(z, y) = - zy + f(z)  \\quad \n",
    "$$\n",
    "So the derivative with respect to the logit z is\n",
    "$$\n",
    "\\frac{\\partial }{\\partial z}L(z,y) = \\frac{\\partial}{\\partial z}\\left [- zy + f(z)  \\right ] =\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial }{\\partial z} [ \\log(1 + e^{z}) ] - y =\n",
    "$$\n",
    "$$\n",
    "\\frac{e^{z}}{1+e^{z}}-y\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Gradients with respect to network parameters\n",
    "\n",
    "Compute gradients for the weights of the Jordan network analytically. That is, derive backpropagation through time for the Jordan network. To do this, it is crucial to first find the derivative w.r.t. the network outputs $\\psi^\\top(t) = \\partial L / \\partial z(t)$ and hidden pre-activations $\\delta^\\top(t) = \\partial L / \\partial s(t)$. We use the shorthand notations $L = \\sum_{t=1}^T L(t)$ and $L(t) = L(y(t), \\hat y(t))$ for convenience. We use numerator-layout notation for partial derivatives (like in the lecture notes) which lets us multiply the chain rule from left to right. \n",
    "\n",
    "Compute the gradients $\\psi^\\top(t), \\delta^\\top(t), \\nabla_W L, \\nabla_R L, \\nabla_V L$. *Hint:* Take a look at the  graph from the previous exercise to see the functional dependencies. \n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "\n",
    "$\\psi^\\top(t) = \\partial L / \\partial z(t) = \\frac{\\partial L(t)}{\\partial \\hat y(t)} \\frac{\\partial \\hat y(t)}{\\partial z(t)}$ is this the thing I calculated in EX5?\n",
    "is this the e(t) that is described in the exercise slides?\n",
    "\n",
    "### For $\\nabla_V L$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\upsilon_{ik}} = \\sum_{t=1}^{T}\\frac{\\partial L(t)}{\\partial \\upsilon_{ik}} = \\sum_{t=1}^{T}\\frac{\\partial L(t)}{\\partial \\hat y(t)}\\frac{\\partial \\hat y(t)}{\\partial z(t)} \\frac{ \\partial  z(t)}{\\partial \\upsilon_{ik}}$\n",
    "\n",
    "and with respect to k:\n",
    "\n",
    "$\\sum_{t=1}^{T}\\frac{\\partial L(t)}{\\partial \\hat y_{k}(t)}\\frac{\\partial \\hat y_{k}(t)}{\\partial z_k(t)} \\frac{ \\partial  z_k(t)}{\\partial \\upsilon_{ik}}\n",
    "$\n",
    "\n",
    "\n",
    "Now because $z(t) = V a(t)$ and thus $z_{k}(t) = \\sum_{i=1}^{I} \\upsilon_{ik}a_{i}(t)$ :\n",
    "\n",
    "$\\frac{ \\partial  z_k(t)}{\\partial \\upsilon_{ik}} =  \n",
    "\\sum_{j=1}^{I} \\frac {\\partial \\upsilon_{jk}a_{j}(t)}{\\partial \\upsilon_{ik}}\n",
    "=a_{i}(t)$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\upsilon_{ik}} = \\sum_{t=1}^{T}\\frac{\\partial L(t)}{\\partial \\hat y_{k}(t)}\\frac{\\partial \\hat y_{k}(t)}{\\partial z_k(t)} a_{i}\n",
    "$\n",
    "\n",
    "If this is applied to the Matrix $V^{I \\times K}$ and $\\psi^\\top(t) = \\partial L / \\partial z(t) = \\frac{\\partial L(t)}{\\partial \\hat y(t)} \\frac{\\partial \\hat y(t)}{\\partial z(t)} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial L(t)}{\\partial \\hat y_{1}(t)}\\frac{\\partial \\hat y_{1}(t)}{\\partial z_1(t)}\n",
    "\\\\ \n",
    "\\vdots \n",
    "\\\\ \n",
    "\\frac{\\partial L(t)}{\\partial \\hat y_{K}(t)}\\frac{\\partial \\hat y_{K}(t)}{\\partial z_K(t)}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial V} = \\nabla_V L = \\sum_{t=1}^{T} \\psi(t) a(t)^{\\top}$\n",
    "\n",
    "\n",
    "### For $\\delta^\\top(t)$\n",
    "\n",
    "The loss that is caused in the future by setting the hidden neurons to a value\n",
    "\n",
    "$\\delta^\\top(t) = \\partial L / \\partial s(t) = \\frac {\\partial L}{\\partial a(t)} \\frac{\\partial a(t)}{\\partial s(t)}$\n",
    "\n",
    "this is because:\n",
    "\n",
    "$a(t) = \\tanh(s(t))$\n",
    "\n",
    "then:\n",
    "\n",
    "$\\partial L / \\partial s(t) = \\left ( \\psi(t) \\frac {\\partial z (t)} {\\partial a(t)} + \\frac{\\partial L}{\\partial s(t+1)}\\frac{\\partial s(t+1)}{\\partial a(t)} \\right )\\frac{\\partial a(t)}{\\partial s(t)}$\n",
    "\n",
    "\n",
    "for similar reasons in the case of the first term $\\psi(t) \\frac {\\partial z (t)} {\\partial a(t)}$ because of $z(t) = V a(t)$. The term that is dependent on t in $s(t)$ propagates through $a(t)$.\n",
    "\n",
    "For the term dependent on $t+1$ it is complicated bc. the outcome of $s(t)$ depends on $s(t-1)$. The derivative therefore has to be calculated recursively for all $t \\in T $ \n",
    "\n",
    "anscheinend $\\frac{\\partial a(t)}{\\partial s(t)} = diag(f{}'(s(t)))$ ?\n",
    "\n",
    "$\\delta(t) = \\left( \\psi(t) V^{\\top} + \\delta(t+1)^{\\top} R^{\\top} \\right ) diag(f{}'(s(t)))$\n",
    "\n",
    "### For R and W\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{ \\partial r_{ij}} = \\sum_{t=1}^{T} \\frac{\\partial L}{ \\partial r_{ij}(t)}=\\sum_{t=1}^{T} \\frac{\\partial L}{\\partial s(t)}\\frac{\\partial s(t)}{ \\partial r_{ij}(t)}$\n",
    "\n",
    "$= \\sum_{t=1}^{T} \\frac{\\partial L}{\\partial s_j(t)}\\frac{\\partial s_j(t)}{ \\partial r_{ij}(t)}\n",
    "= \\sum_{t=1}^{T} \\delta_{j}(t) a_{i}(t-1)$\n",
    "\n",
    "st.\n",
    "\n",
    "$\\nabla_R L = \\frac{\\partial L}{\\partial R} = \\sum_{t=1}^{T} \\delta(t) a(t-1)$\n",
    "\n",
    "And similar to R:\n",
    "\n",
    "$\\frac{\\partial L}{ \\partial \\omega_{dj}} = \\sum_{t=1}^{T} \\delta_{j}(t)x_{d}(t)$\n",
    "\n",
    "st.\n",
    "\n",
    "$\\nabla_W L = \\frac{\\partial L}{ \\partial W} = \\sum_{t=1}^{T} \\delta(t)x(t)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: The backward pass\n",
    "Write a function `backward` that takes a model `self` as argument. The function should compute the gradients of the loss with respect to all model parameters and store them to `model.dW`, `model.dR`, `model.dV`, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "    self.psi = \n",
    "    self.delta = \n",
    "    \n",
    "    self.dV = \n",
    "    \n",
    "    \n",
    "Obj.backward = backward\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8: Gradient checking\n",
    "Write a function `grad_check` that takes a model `self`, a float `eps` and another float `thresh` as arguments and computes the numerical gradients of the model parameters according to the approximation\n",
    "$$\n",
    "f'(x) \\approx \\frac{f(x + \\varepsilon) - f(x - \\varepsilon)}{2 \\varepsilon}.\n",
    "$$\n",
    "If any of the analytical gradients are farther than `thresh` away from the numerical gradients the function should throw an error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_check(self, eps, thresh):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "Obj.grad_check = grad_check\n",
    "model.grad_check(1e-7, 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9: Parameter update\n",
    "\n",
    "Write a function `update` that takes a `model` and a float argument `eta`, which represents the learning rate. The method should implement the gradient descent update rule $\\theta \\gets \\theta - \\eta \\nabla_{\\theta}L$ for all model parameters $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, eta):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "Obj.update = update\n",
    "model.update(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10: Data generation\n",
    "\n",
    "There are two classes, both occurring with probability 0.5. There is one input unit. Only the first sequence element conveys relevant information about the class. Sequence elements at positions $t > 1$ are generated by a Gaussian with mean zero and variance 0.2. The first sequence element is 1.0 (-1.0) for class 1 (2). Target at sequence end is 1.0 (0.0) for class 1 (2)\n",
    "\n",
    "Write a function `generate_data` that takes an integer `T` as argument which represents the sequence length. Seed the `numpy` random generator with the number `0xDEADBEEF`. Implement the Python generator pattern and produce data in the way described above. The input sequences should have shape `(T, 1)` and the target values should have shape `(1,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(T):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "data = generate_data(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 11: Network training\n",
    "\n",
    "Train a Jordan network with 32 hidden units. Start with input sequences of length one and tune the leraning rate and the number of update steps. Then increase the sequence length by one and tune the hyperparameters again. What is the maximal sequence length for which the Jordan network can achieve a performance that is better than random? Visualize your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
