{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: The Jordan Network\n",
    "\n",
    "*Author:* Thomas Adler\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors.\n",
    "\n",
    "## Exercise 1: Initializing the network\n",
    "Consider the Jordan network (with $f(x) = \\tanh(x) = (e^x - e^{-x})(e^x + e^{-x})^{-1}$ and $\\varphi(x) = \\sigma(x) = (1+e^{-x})^{-1}$ and transposed weight matrices compared to the lecture notes)\n",
    "$$\n",
    "s(t) = W x(t) + R \\hat y(t-1) \\\\\n",
    "a(t) = \\tanh(s(t)) \\\\\n",
    "z(t) = V a(t) \\\\\n",
    "\\hat y(t) = \\sigma(z(t))\n",
    "$$\n",
    "for $t \\in \\mathbb{N}, x(t) \\in \\mathbb{R}^{D}, s(t) \\in \\mathbb{R}^{I}, a(t) \\in \\mathbb{R}^{I}, z(t) \\in \\mathbb{R}^K, \\hat y(t) \\in \\mathbb{R}^K$ and $W, R, V$ are matrices of appropriate sizes and $\\hat y(0) = 0$. \n",
    "\n",
    "Write a function `init` that takes a `model` and integers $D, I, K$ as arguments and stores the matrices $W, R, V$ as members `model.W`, `model.R`, `model.V`, respectively. The matrices should be `numpy` arrays of appropriate sizes and filled with random values that are uniformly distributed between -0.01 and 0.01. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W:\n",
      " [[-0.00316844  0.00030774 -0.00161916 -0.00480986  0.00498588]\n",
      " [ 0.00693369  0.00548952  0.00025712 -0.00368635 -0.00263879]\n",
      " [ 0.00211893 -0.00808799  0.00036712  0.00971809  0.00211446]]\n",
      "R:\n",
      " [[ 0.00060102]\n",
      " [ 0.0006513 ]\n",
      " [-0.00665671]\n",
      " [ 0.00165651]\n",
      " [ 0.00696615]]\n",
      "V:\n",
      " [[-0.00154766]\n",
      " [ 0.00760051]\n",
      " [ 0.00126295]\n",
      " [ 0.00403284]\n",
      " [ 0.00757331]]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "def tanh_der(s):\n",
    "    return np.ones_like(s)-np.square(np.tanh(s))\n",
    "\n",
    "class Gate(object):\n",
    "    pass\n",
    "\n",
    "class Obj(object):\n",
    "    pass\n",
    "\n",
    "def init(model, D, I, K): #dunder pls\n",
    "    model.W = np.random.uniform(low=-0.01,high=0.01,size=(D,I))\n",
    "    model.R = np.random.uniform(low=-0.01,high=0.01,size=(I,K))\n",
    "    model.V = np.random.uniform(low=-0.01,high=0.01,size=(I,K))\n",
    "\n",
    "    t = 5 #any value of t\n",
    "    test_x = np.random.uniform(size=(D))\n",
    "    try:\n",
    "        model.W.T @ test_x\n",
    "        model.R.T*np.tanh(t)\n",
    "        model.V.T*np.tanh(t)\n",
    "    except Exception as e:\n",
    "        print(f\"Model init fail: {e}\")\n",
    "        \n",
    "Obj.init=init\n",
    "\n",
    "T, D, I, K = 10, 3, 5, 1\n",
    "\n",
    "model = Obj()\n",
    "model.init(D, I, K)\n",
    "print(\"W:\\n\",model.W)\n",
    "print(\"R:\\n\",model.R)\n",
    "print(\"V:\\n\",model.V)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Numerical stability of the binary cross-entropy loss function\n",
    "\n",
    "We will use the binary cross-entropy loss function to train our RNN, which is defined as \n",
    "$$\n",
    "L(\\hat y, y) = -y \\log \\hat y - (1-y) \\log (1-\\hat y) \\quad \\text{where} \\quad \\hat y = \\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "is the sigmoid function. Its argument $z$ is called *logit*. For reasons of numerical stability it is better to let the model emit the logit and incorporate the sigmoid function into the loss function. Explain why this is the case and how we can gain numerical stability by combining the two functions into one. *Hint:* Find a numerically stable version of the function $f(z) = \\log(1+e^{z})$ and rewrite $L(z,y)$ in terms of $f(z)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANSWER\n",
    "\n",
    "If the model makes a large error st. z is far below zero, the sigmoid function might put out a float that is small enough to become a numerical zero in the execution of the code. This becomes problematic as the sigmoid output is passed to the loss function where the log of $\\hat y$ becomes $- \\infty$ and thus numerically unstable.\n",
    "We thus integrate the sigmoidal function into the loss function:\n",
    "\n",
    "$$\n",
    "L(z, y) = -y \\log ( \\frac{1}{1+e^{-z}} ) - (1-y) \\log (1-( \\frac{1}{1+e^{-z}} )) \\quad =\n",
    "$$\n",
    "$$\n",
    "= -y\\log(\\frac{1}{1 + e^{-z}}) - (1 - y)\\log(\\frac{e^{-z}}{1 + e^{-z}})\\\\\n",
    "= y  \\log(1 + e^{-z}) + (1 - y)  (-\\log(e^{-z}) + \\log(1 + e^{-z}))\\\\\n",
    "= y  \\log(1 + e^{-z}) + (1 - y)  (z + \\log(1 + e^{-z})\\\\\n",
    "= (1 - y)  z + \\log(1 + e^{-z})\\\\\n",
    "= z - z  y + \\log(1 + e^{-z})\\\\\n",
    "$$\n",
    "\n",
    "If z is negative and large the output of $e^{-z}$ can cause numerical overflow.\n",
    "In order to prevent this we can:\n",
    "\n",
    "$$\n",
    "  z - zy + \\log(1 + e^{-z})\\\\\n",
    "= \\log(e^{z}) - zy + \\log(1 + e^{-z})\\\\\n",
    "= - zy + \\log(1 + e^{z})\\\\\n",
    "$$\n",
    "\n",
    "rewriting it in terms of $f(z)=\\log (1+e^z)$:\n",
    "\n",
    "$$\n",
    "= - zy + f(z)\\\\\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: The forward pass\n",
    "Write a function `forward` that takes a `model`, a sequence of input vectors $(x(t))_{t=1}^T$, and a label `y` as arguments. The inputs will be represented as a `numpy` array of shape `(T, D)`. It should execute the behavior of the Jordan network and evaluate the numerically stablilized binary cross-entropy loss at the end of the sequence and return the resulting loss value. Store the sequence of hidden activations $(a(t))_{t=1}^T$ and the sequence of logits $(z(t))_{t=1}^T$ into `model.a` and `model.z`, respectively, using the same representation scheme as for the inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_hat,y):\n",
    "\n",
    "    return -(y*np.log(y_hat))-(1-y)*(np.log(1-y_hat))\n",
    "\n",
    "def forward(self, x, y):\n",
    "    self.y = y\n",
    "    self.x = x\n",
    "    self.a = np.zeros((T+1,I))#initial a is zeros bc there is no x for it\n",
    "    self.z = np.zeros((T+1,K))#because there is one z at t=-1\n",
    "    self.z[0]=self.a[0]@self.V\n",
    "\n",
    "    self.y_hat = None\n",
    "    self.s = np.empty((T,I))\n",
    "    \n",
    "    for t,x_t in enumerate(x):#defining t to start at one but keeping a zero-th a makes this tricky\n",
    "\n",
    "        self.s[t] = self.W.T@x[t] + self.R@sigmoid(self.a[t]@self.V)\n",
    "        #z[t] is z[t-1] in math notation bc z starts one timestampt earlier so z[t] is z[t-1], same thing for a[t]\n",
    "        #in jordan network a[t-1] is replaced by y_hat[t-1]\n",
    "        self.a[t+1]=np.tanh(self.s[t])\n",
    "        self.z[t+1]=self.a[t]@self.V #a[-1] is now the new a, or a(t)\n",
    "    self.y_hat=sigmoid(self.z)\n",
    "    loss_sequence = np.apply_along_axis(loss_fn,0,self.y_hat,y=y)\n",
    "    return loss_sequence[-1]\n",
    "\n",
    "Obj.forward = forward\n",
    "model.forward(np.random.uniform(-1, 1, (T, D)), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: The computational graph\n",
    "\n",
    "Visualize the functional graph of the Jordan network unfolded in time. The graph should show at least 3 consecutive time steps. Use the package `networkx` in combination with `matplotlib` to draw a directed graph with labelled nodes and edges. If you need help take a look at [this guide](https://networkx.guide/visualization/basics/). Make sure to arrange the nodes in a meaningful way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "G=nx.DiGraph()\n",
    "\n",
    "positions={}\n",
    "edge_labels={}\n",
    "for _ in range(1,6):\n",
    "    G.add_edge(f\"yhat({_-1})\",f\"a({_})\")\n",
    "    positions[f\"a({_})\"]=[_+1,0]\n",
    "    edge_labels[(f\"yhat({_-1})\",f\"a({_})\")]=\"R\"\n",
    "    G.add_edge(f\"x({_})\",f\"a({_})\")\n",
    "    positions[f\"x({_})\"]=[_+1,-1]\n",
    "    edge_labels[(f\"x({_})\",f\"a({_})\")]=\"W\"\n",
    "    G.add_edge(f\"a({_})\",f\"yhat({_})\")\n",
    "    positions[f\"yhat({_-1})\"]=[_,+1]\n",
    "    edge_labels[(f\"a({_})\",f\"yhat({_})\")]=\"V\"\n",
    "              \n",
    "positions[f\"yhat({5})\"]=[_+1,+1]    \n",
    "\n",
    "nx.draw_networkx_edge_labels(G,pos=positions,edge_labels=edge_labels,font_color='blue')\n",
    "\n",
    "nx.draw(G,with_labels=True,pos=positions,node_size=1750,node_color=\"#C1C2D4\",alpha =1,edge_color='k',linewidths=1)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Derivative of the loss\n",
    "\n",
    "Calculate the derivative of the binary cross-entropy loss function $L(z, y)$ with respect to the logit $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANSWER\n",
    "\n",
    "Binary cross-entropy loss function is\n",
    "$$\n",
    "L(z, y) = - zy + f(z)  \\quad \n",
    "$$\n",
    "So the derivative with respect to the logit z is\n",
    "$$\n",
    "\\frac{\\partial }{\\partial z}L(z,y) = \\frac{\\partial}{\\partial z}\\left [- zy + f(z)  \\right ] =\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial }{\\partial z} [ \\log(1 + e^{z}) ] - y =\n",
    "$$\n",
    "$$\n",
    "\\frac{e^{z}}{1+e^{z}}-y\n",
    "$$\n",
    "Now because $\\sigma(z) = \\frac{1}{1+e^-z} = \\frac{e^z}{1+e^z}$\n",
    "\n",
    "$$\n",
    "=\\sigma(z)-y  \n",
    "$$\n",
    "$$\n",
    "= \\hat{y}-y\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Gradients with respect to network parameters\n",
    "\n",
    "Compute gradients for the weights of the Jordan network analytically. That is, derive backpropagation through time for the Jordan network. To do this, it is crucial to first find the derivative w.r.t. the network outputs $\\psi^\\top(t) = \\partial L / \\partial z(t)$ and hidden pre-activations $\\delta^\\top(t) = \\partial L / \\partial s(t)$. We use the shorthand notations $L = \\sum_{t=1}^T L(t)$ and $L(t) = L(y(t), \\hat y(t))$ for convenience. We use numerator-layout notation for partial derivatives (like in the lecture notes) which lets us multiply the chain rule from left to right. \n",
    "\n",
    "Compute the gradients $\\psi^\\top(t), \\delta^\\top(t), \\nabla_W L, \\nabla_R L, \\nabla_V L$. *Hint:* Take a look at the  graph from the previous exercise to see the functional dependencies. \n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "## $\\psi^\\top(t)$\n",
    "\n",
    "$\\psi^\\top(t) = \\frac {\\partial L} { \\partial z(t)} =\\sigma(z(t))-y$\n",
    "\n",
    "## $\\delta^\\top(t)$\n",
    "\n",
    "The loss that is caused in the future by setting the hidden neurons a(t)\n",
    "\n",
    "$$\n",
    "%\\delta^\\top(t) = \\partial L / \\partial s(t) = \\frac {\\partial L}{\\partial \\hat{y}(t)} \\frac{\\partial \\hat{y}(t)}{\\partial s(t)}\\\\\n",
    "%= \\left ( \\frac{\\partial L(y(t),\\hat{y}(t))}{\\partial \\hat{y}(t)} + \\frac{\\partial L}{\\partial s(t+1)} \\frac{\\partial s(t+1)}{\\partial \\hat{y}(t)} \\right) \\frac{\\partial \\hat{y}(t)}{\\partial s(t)}\\\\\n",
    "%=\\left( \\frac{\\partial L}{\\partial z(t)}\\frac{\\partial z(t)}{\\partial \\hat{y}(t)} + \\frac{\\partial L}{\\partial s(t+1)} \\frac{\\partial s(t+1)}{\\partial \\hat{y}(t)}  \\right) \\frac{\\partial \\hat{y}(t)}{\\partial s(t)}\\\\\n",
    "%=\\left ( \\psi^\\top(t) V + \\delta(t+1)^\\top R^\\top \\right)diag({f}'(s(t)))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta^\\top(t) = \\partial L / \\partial s(t) = \\frac {\\partial L}{\\partial \\hat{y}(t)} \\frac{\\partial \\hat{y}(t)}{\\partial z(t)} \\frac{ \\partial z(t)}{\\partial a(t)} \\frac {\\partial a(t)}{s(t)}\\\\\n",
    "= \\left ( \\psi (t) V^{\\top} + \\frac{\\partial L}{\\partial s(t+1)} \\frac{\\partial s(t+1)}{\\partial \\hat{y}(t)} \\right) \\frac{\\partial a(t)}{\\partial s(t)}\\\\\n",
    "=\\left ( \\psi(t) V^{\\top} + \\delta(t+1)^\\top R^\\top \\right) diag \\left ({f}'(s(t)) \\right )\n",
    "$$\n",
    "\n",
    "${f}'(s(t))$ being the derivative of the $\\tanh$ function\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\tanh(s(t))}{\\partial (s(t))} = 1 - \\tanh(s(t))^2$\n",
    "\n",
    "### For $\\nabla_V L$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial V} = \\sum_{t=1}^{T}\\frac{\\partial L(t)}{\\partial  z(t)} \\frac{\\partial  z(t)}{\\partial V}$\n",
    "\n",
    "\n",
    "Now because $z(t) = V^\\top a(t)$ :\n",
    "\n",
    "$\\frac{ \\partial  z(t)}{\\partial V} =  \n",
    "\\frac {\\partial(V^\\top a(t))}{\\partial V}\n",
    "=a(t)$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\upsilon_{i}} = \\sum_{t=1}^{T}\\frac{\\partial L(t)}{\\partial z(t)} a(t)\n",
    "$\n",
    "\n",
    "We already know that \n",
    "\n",
    "$\n",
    "\\frac{\\partial L(t)}{\\partial z(t)} = \\psi^\\top(t)\n",
    "$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial V} = \\nabla_V L = \\sum_{t=1}^{T} \\psi(t) a(t)^{\\top}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### For R and W\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{ \\partial R} = \\sum_{t=1}^{T} \\frac{\\partial L}{ \\partial R}=\\sum_{t=1}^{T} \\frac{\\partial L}{\\partial s(t)}\\frac{\\partial s(t)}{ \\partial R}$\n",
    "\n",
    "now because \n",
    "\n",
    "$\\frac{\\partial L}{\\partial s(t)} = \\delta(t)$\n",
    "\n",
    "and \n",
    "\n",
    "$\\frac{\\partial s(t)}{ \\partial R} = \\hat{y}(t-1)$\n",
    "\n",
    "$\\nabla_R L$ can be rewritten as:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial R} = \\sum_{t=1}^{T} \\delta(t) \\hat{y}(t-1)$\n",
    "\n",
    "And similar to R:\n",
    "\n",
    "$\\frac{\\partial L}{ \\partial W} = \\sum_{t=1}^{T} \\delta(t) \\frac{\\partial s(t)}{\\partial{W}}$\n",
    "\n",
    "because \n",
    "\n",
    "$\\frac{\\partial s(t)}{\\partial{W}} = x(t)$\n",
    "\n",
    "$\\nabla_W L$ can be rewritten as:\n",
    "\n",
    "$\\frac{\\partial L}{ \\partial W} = \\sum_{t=1}^{T} \\delta(t)x(t)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: The backward pass\n",
    "Write a function `backward` that takes a model `self` as argument. The function should compute the gradients of the loss with respect to all model parameters and store them to `model.dW`, `model.dR`, `model.dV`, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig_der(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def backward(self):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "    self.psi = np.zeros_like(self.z[1:])\n",
    "    self.psi[-1]=sigmoid(self.z[-1])-self.y\n",
    "    \n",
    "    #self.psi = self.y_hat[1:]-self.y\n",
    "    \n",
    "    self.delta = np.zeros((T+1,I))#might be I,T\n",
    "\n",
    "\n",
    "    for t_shift in range(1,T+1):\n",
    "\n",
    "        self.delta[-(t_shift+1)] = (\n",
    "                                        (\n",
    "                                            self.psi[-t_shift]\n",
    "                                            +(self.delta[-t_shift]@self.R)*sig_der(self.z[-t_shift])\n",
    "                                        )\n",
    "                                        * (self.V.T[0]@np.diag(tanh_der(self.s[-t_shift])))\n",
    "                                   )\n",
    "    self.dV = np.array([sum([np.array(self.a[t]).T*self.psi[t] for t in range(T)])])\n",
    "    self.dR = np.array([sum([self.delta[t]*self.y_hat[t] for t in range(T)])])\n",
    "    self.dW = sum([np.outer(self.delta[t],self.x[t]) for t in range(T)]).T\n",
    "    \n",
    "Obj.backward = backward\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8: Gradient checking\n",
    "Write a function `grad_check` that takes a model `self`, a float `eps` and another float `thresh` as arguments and computes the numerical gradients of the model parameters according to the approximation\n",
    "$$\n",
    "f'(x) \\approx \\frac{f(x + \\varepsilon) - f(x - \\varepsilon)}{2 \\varepsilon}.\n",
    "$$\n",
    "If any of the analytical gradients are farther than `thresh` away from the numerical gradients the function should throw an error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_check(self, eps, thresh):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "    x,y=np.random.uniform(-1, 1, (T, D)), 1\n",
    "    \n",
    "    W_store=self.W\n",
    "    num_grad_W=np.zeros_like(self.W)\n",
    "    for col_num_grad,col_W in zip(num_grad_W,self.W):\n",
    "        for row_idx in range(col_W.shape[0]):\n",
    "            #negative eps\n",
    "            col_W[row_idx] =col_W[row_idx]-eps\n",
    "            out_low =self.forward(x,y)\n",
    "            col_W[row_idx] =col_W[row_idx]+2*eps\n",
    "            out_high=self.forward(x,y)\n",
    "            col_num_grad[row_idx]=(out_high-out_low)/(2*eps)\n",
    "            self.W=W_store\n",
    "    \n",
    "    self.backward()\n",
    "    \n",
    "    #print('dW',self.dW.T)\n",
    "    #print('num grad', num_grad_W.T)   \n",
    "    grad_err_W=np.abs(self.dW.T-num_grad_W.T)\n",
    "    #print('grad err', grad_err_W)\n",
    "\n",
    "\n",
    "    \n",
    "    R_store=self.R\n",
    "    num_grad_R=np.zeros_like(self.R)\n",
    "    for col_num_grad,col_R in zip(num_grad_R,self.R):\n",
    "        for row_idx in range(col_R.shape[0]):\n",
    "            #negative eps\n",
    "            col_R[row_idx] =col_R[row_idx]-eps\n",
    "            out_low =self.forward(x,y)\n",
    "            col_R[row_idx] =col_R[row_idx]+2*eps\n",
    "            out_high=self.forward(x,y)\n",
    "            col_num_grad[row_idx]=(out_high-out_low)/(2*eps)\n",
    "            self.R=R_store\n",
    "    \n",
    "    self.backward()\n",
    "    \n",
    "    #print('dR',self.dR.T.shape)\n",
    "    #print('num grad', num_grad_R.shape)    \n",
    "    grad_err_R=np.abs(np.subtract(self.dR,num_grad_R.T[0]))\n",
    "    #print('grad err', grad_err_R)\n",
    "\n",
    "    V_store=self.V\n",
    "    num_grad_V=np.zeros_like(self.V)\n",
    "    for col_num_grad,col_V in zip(num_grad_V,self.V):\n",
    "        for row_idx in range(col_V.shape[0]):\n",
    "            #negative eps\n",
    "            col_V[row_idx] =col_V[row_idx]-eps\n",
    "            out_low =self.forward(x,y)\n",
    "            col_V[row_idx] =col_V[row_idx]+2*eps\n",
    "            out_high=self.forward(x,y)\n",
    "            col_num_grad[row_idx]=(out_high-out_low)/(2*eps)\n",
    "            self.V=V_store\n",
    "    \n",
    "    self.backward()\n",
    "    \n",
    "    #print('dV',self.dV.T)\n",
    "    #print('num grad', num_grad_V.T)\n",
    "    grad_err_V=np.abs(np.subtract(self.dV.T,num_grad_V))\n",
    "    #print('grad err', grad_err_V)\n",
    "    \n",
    "\n",
    "    np.testing.assert_allclose(self.dV.T,num_grad_V,atol=thresh)\n",
    "    np.testing.assert_allclose(self.dR.T,num_grad_R,atol=thresh)\n",
    "    np.testing.assert_allclose(self.dW.T,num_grad_W.T,atol=thresh)\n",
    "    \n",
    "Obj.grad_check = grad_check\n",
    "\n",
    "model.grad_check(1e-7, 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9: Parameter update\n",
    "\n",
    "Write a function `update` that takes a `model` and a float argument `eta`, which represents the learning rate. The method should implement the gradient descent update rule $\\theta \\gets \\theta - \\eta \\nabla_{\\theta}L$ for all model parameters $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, eta):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "    self.backward()\n",
    "    self.W+=eta*self.dW\n",
    "    self.R+=eta*self.dR.T\n",
    "    self.V+=eta*self.dV.T\n",
    "\n",
    "Obj.update = update\n",
    "model.update(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10: Data generation\n",
    "\n",
    "There are two classes, both occurring with probability 0.5. There is one input unit. Only the first sequence element conveys relevant information about the class. Sequence elements at positions $t > 1$ are generated by a Gaussian with mean zero and variance 0.2. The first sequence element is 1.0 (-1.0) for class 1 (2). Target at sequence end is 1.0 (0.0) for class 1 (2)\n",
    "\n",
    "Write a function `generate_data` that takes an integer `T` as argument which represents the sequence length. Seed the `numpy` random generator with the number `0xDEADBEEF`. Implement the Python generator pattern and produce data in the way described above. The input sequences should have shape `(T, 1)` and the target values should have shape `(1,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0xDEADBEEF)#is it supposed to spell dead beef? moooh!\n",
    "\n",
    "def generate_data(T):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "    x_prime=np.random.choice([1,-1],1,p=[.5,.5])\n",
    "    gaussian=np.random.normal(0,.2,size=(1,(T-1)))\n",
    "    x = np.insert(gaussian,0,x_prime.T,axis=1)\n",
    "    y = ((x_prime+1)/2).astype(int)#hacky\n",
    "    yield [x.T,y]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 11: Network training\n",
    "\n",
    "Train a Jordan network with 32 hidden units. Start with input sequences of length one and tune the leraning rate and the number of update steps. Then increase the sequence length by one and tune the hyperparameters again. What is the maximal sequence length for which the Jordan network can achieve a performance that is better than random? Visualize your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
